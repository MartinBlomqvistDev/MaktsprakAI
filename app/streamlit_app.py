# =========================================================
# Fil: streamlit_app.py
# Syfte: Interaktiv dashboard f√∂r Maktspr√•kAI
# =========================================================

import sys
from pathlib import Path
import pandas as pd
import streamlit as st
import plotly.express as px
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from datetime import datetime, timedelta, date
import random
import re
from streamlit_option_menu import option_menu

import feedparser
from bs4 import BeautifulSoup
import requests
from huggingface_hub import hf_hub_download

plt.rcParams['font.family'] = 'sans-serif'

# =====================
# S√∂kv√§gshantering
# =====================
proj_root = Path(__file__).parent.parent.resolve()
if str(proj_root) not in sys.path:
    sys.path.insert(0, str(proj_root))

# =====================
# Importer
# =====================
from src.maktsprak_pipeline.db import (
    fetch_speeches_count,
    fetch_latest_speech_date_cached,
    fetch_random_speeches,
    fetch_speeches_historical,
    insert_speech,
    insert_tweet
)
from src.maktsprak_pipeline.nlp import apply_ton_lexicon, combined_stopwords, clean_text
from src.maktsprak_pipeline.model import load_model_and_tokenizer, predict_party

# =====================
# App-inst√§llningar
# =====================
st.set_page_config(
    page_title="Maktspr√•kAI",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =====================
# Konstanter
# =====================
PARTY_ORDER = ["V", "MP", "S", "C", "L", "KD", "M", "SD"] 
PAGE_OPTIONS = ["Om projektet", "Partiprediktion", "Spr√•kbruk & Retorik", "Evaluering", "Historik"]

# =====================
# Helper-funktioner
# =====================
def preprocess_for_wordcloud(text_blob: str, min_length: int = 3) -> str:
    words = re.sub(r'[^a-zA-Z√•√§√∂√Ö√Ñ√ñ\s]', '', text_blob).lower().split()
    filtered_words = [word for word in words if word not in combined_stopwords and len(word) >= min_length]
    return " ".join(filtered_words)

@st.cache_data(ttl=900)
def fetch_news(feed_url="http://www.svt.se/nyheter/inrikes/rss.xml"):
    feed = feedparser.parse(feed_url)
    news_items = []
    for entry in feed.entries[:5]:
        news_items.append({
            "title": entry.title,
            "link": entry.link,
            "published": entry.published
        })
    return news_items

@st.cache_data(ttl=3600)
def get_full_article_text(url: str) -> str:
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        
        possible_containers = [
            soup.find(class_='c-article__content'),
            soup.find('article'),
            soup.find(class_='article-body'),
            soup.find(class_='entry-content'),
            soup.find(class_='td-post-content'),
            soup.find('main')
        ]
        main_content = next((c for c in possible_containers if c), None)
        if main_content:
            for unwanted in main_content.find_all(['figure', 'figcaption', 'script', 'aside', 'header', 'footer']):
                unwanted.decompose()
            paragraphs = main_content.find_all('p')
            full_text = " ".join([p.get_text(strip=True) for p in paragraphs])
            return full_text.strip()
        return ""
    except Exception as e:
        print(f"Ett fel uppstod vid skrapning av {url}: {e}")
        return ""

@st.cache_data(ttl=1800, show_spinner=False)
def fetch_party_articles(articles_per_party: int = 1):
    party_feeds = {
        "S": "https://via.tt.se/rss/releases/latest?publisherId=142377",
        "M": "https://moderaterna.se/feed/",
        "SD": "https://sd.se/feed/",
        "C": "https://via.tt.se/rss/releases/latest?publisherId=3237070",
        "V": "https://vansterpartiet.se/feed/",
        "KD": "https://via.tt.se/rss/releases/latest?publisherId=3236814",
        "L": "https://www.liberalerna.se/feed/",
        "MP": "https://via.tt.se/rss/releases/latest?publisherId=3237031"
    }
    
    all_valid_articles = []
    debug_log = []
    
    for party, url in party_feeds.items():
        found_for_party_count = 0
        entries = []
        try:
            feed = feedparser.parse(url)
            entries = feed.entries
            if not entries and party == "S":
                debug_log.append(f"INFO [S]: RSS tomt. K√∂r special-skrapa f√∂r S nyhetssida.")
                try:
                    s_url = "https://www.socialdemokraterna.se/nyheter/nyheter"
                    response = requests.get(s_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                    s_soup = BeautifulSoup(response.content, "html.parser")
                    article_cards = s_soup.find_all('a', class_='c-card')
                    for card in article_cards[:10]:
                        title = card.find('h3', class_='c-card__title')
                        if title and card.has_attr('href'):
                            full_link = card['href']
                            if full_link.startswith('/'):
                                full_link = "https://www.socialdemokraterna.se" + full_link
                            entries.append({"title": title.get_text(strip=True), "link": full_link})
                except Exception as e:
                    debug_log.append(f"CRITICAL [S]: Special-skrapan misslyckades: {e}")

            debug_log.append(f"INFO [{party}]: Hittade {len(entries)} inl√§gg att bearbeta.")
            
            for i, entry in enumerate(entries):
                if found_for_party_count >= articles_per_party:
                    break
                title = entry.get('title', "Titel saknas")
                link = entry.get('link', None)
                if not link: continue
                debug_log.append(f"  -> F√∂rs√∂ker h√§mta artikel {i+1}: '{title}'")
                full_content = get_full_article_text(link)
                if not full_content or len(full_content) < 250:
                    debug_log.append(f"    - MISSLYCKADES: Skrapan hittade f√∂r lite text (<250 tecken).")
                    continue
                if is_unwanted_content(title, full_content):
                    debug_log.append(f"    - MISSLYCKADES: Inneh√•llet flaggades som 'o√∂nskat'.")
                    continue
                debug_log.append(f"    - OK: Artikeln godk√§ndes.")
                found_for_party_count += 1
                all_valid_articles.append({ "title": title, "link": link, "content": full_content, "true_party": party })
        except Exception as e:
            debug_log.append(f"CRITICAL [{party}]: Ett allvarligt fel intr√§ffade: {e}")
            
    random.shuffle(all_valid_articles)
    return {"articles": all_valid_articles, "log": debug_log}

def is_unwanted_content(title: str, content: str) -> bool:
    title_lower = title.lower()
    content_lower = content.lower()
    text_length = len(content)
    announcement_keywords = ["v√§lkommen till", "bjuder in", "schema:", "anm√§lan", "plats:", "program:", "agenda:"]
    if any(k in content_lower for k in announcement_keywords):
        return True
    job_ad_keywords = ["jobba hos oss", "s√∂ker", "ans√∂k", "kvalifikationer", "anst√§llning", "rekryterar", "ledig tj√§nst"]
    if any(k in title_lower or k in content_lower[:500] for k in job_ad_keywords):
        return True
    weak_filter_keywords = ["video:", "live:", "se talet", "anf√∂rande", "fr√•gestund", "turn√©", "bes√∂ker"]
    if any(k in title_lower for k in weak_filter_keywords) and text_length < 500:
        return True
    return False

# =====================
# Ladda modell, tokenizer och lexikon
# =====================
@st.cache_resource(show_spinner="Laddar AI-modell och lexikon...")
def load_all_resources():
    model, tokenizer = load_model_and_tokenizer() 
    lexicon_local_path = hf_hub_download(
        repo_id="MartinBlomqvist/maktsprak_bert",
        filename="politisk_ton_lexikon.csv",
        revision="main"
    )
    return model, tokenizer, Path(lexicon_local_path)

model, tokenizer, LEXICON_PATH = load_all_resources()

# =====================
# Gemensam och cachad funktion f√∂r all evaluering
# =====================
@st.cache_data(ttl=60)
def get_data_signature():
    count = fetch_speeches_count()
    latest_date = fetch_latest_speech_date_cached()
    return (count, latest_date)

@st.cache_data(show_spinner="V√§rmer upp AI-modellen...")
def run_live_evaluation(articles_per_party: int = 5):
    fetch_results = fetch_party_articles(articles_per_party=articles_per_party)
    articles_to_analyze = fetch_results.get("articles", [])
    
    if not articles_to_analyze:
        return pd.DataFrame(), 0.0, 0

    results = []
    for article in articles_to_analyze:
        cleaned_for_model = clean_text(article['content'])  # <-- √Ñndrat h√§r
        party_probs = predict_party(model, tokenizer, [cleaned_for_model])
        predicted_party = max(party_probs[0].items(), key=lambda x: x[1])[0]
        results.append({
            "Titel": article['title'], 
            "Sant parti": article['true_party'], 
            "Modellens gissning": predicted_party,
            "Korrekt?": (article['true_party'] == predicted_party)
        })
    results_df = pd.DataFrame(results)
    total_count = len(results_df)
    correct_count = results_df["Korrekt?"].sum()
    accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0.0

    return results_df, accuracy, total_count

# =====================
# V√§lkomstsida
# =====================
def welcome_page():
    st.title("Maktspr√•kAI: Den politiska spr√•kkartan")
    st.markdown("Interaktiv AI-analys av partiernas retorik och m√∂nster.")

    # News-box CSS
    st.markdown("""
        <style>
        .news-box {
            border: 1px solid #555;           /* En tunn gr√• ram */
            border-radius: 10px;              /* Mjukt rundade h√∂rn */
            padding: 15px;                    /* Lite luft inuti rutan */
            background-color: transparent;    /* Transparent bakgrund, eller v√§lj en f√§rg t.ex. #1E1E2A */
            margin-bottom: 20px;              /* Lite utrymme under rutan */
        }
        .news-box h3 {
            margin-top: 0;                    /* Tar bort extra utrymme ovanf√∂r rubriken */
            margin-bottom: 10px;
            font-size: 1.25em;                /* En lagom stor rubrik */
        }
        .news-box ul {
            list-style-type: none;            /* Tar bort prickarna i listan */
            padding-left: 0;                  /* Tar bort indraget */
            margin-bottom: 0;
        }
        .news-box li {
            margin-bottom: 8px;               /* Lite avst√•nd mellan varje nyhetsrad */
            font-size: 0.9em;                 /* N√•got mindre text f√∂r nyheterna */
        }
        </style>
    """, unsafe_allow_html=True)

    # # === NY LAYOUT MED TV√Ö KOLUMNER ===
    main_col, news_col = st.columns([2, 1])  # V√§nster kolumn √§r dubbelt s√• bred som den h√∂gra
    with main_col:
        # Dashboarddelen
        st.divider()
        live_results_df, live_accuracy, total_live_articles = run_live_evaluation(articles_per_party=4)
    
        total_speeches = fetch_speeches_count()
        latest_speech_date = fetch_latest_speech_date_cached()

        col1, col2, col3 = st.columns(3)
        col1.metric(f"Tr√§ffs√§kerhet ({total_live_articles} artiklar)", f"{live_accuracy:.1f}%")
        col2.metric("Totalt anf√∂randen i databasen", f"{total_speeches:,}".replace(",", " "))
        col3.metric("Senaste anf√∂rande", latest_speech_date)
    
        st.divider()
        
        st.markdown(
            """
            ### Martin Blomqvist ‚Äì Om mig och projektet

            Jag heter **Martin Blomqvist** och drivs av att f√∂rst√• och f√∂rb√§ttra komplexa system. Min bakgrund √§r bred ‚Äì jag har arbetat i vitt skilda milj√∂er, fr√•n **ekologiskt jordbruk** till avancerad **dataanalys**. Oavsett sammanhang har fokus alltid legat p√• detsamma: att **hitta den dolda strukturen** i kaoset och bygga l√∂sningar som fungerar i den verkliga v√§rlden.
            
            ---
            
            **Maktspr√•kAI** √§r en direkt till√§mpning av dessa erfarenheter. Det √§r ett fullskaligt **data science- och NLP-projekt** som skapades under EC Utbildnings Data Scientist-program. Det visar hur jag kombinerar min systemanalytiska f√∂rm√•ga med teknisk kompetens.

            **Projektets m√•l** √§r att **utforska, analysera och visualisera det politiska spr√•kbruket i Sveriges riksdag** genom att kombinera modern maskininl√§rning och AI med robust systemdesign. Jag tar nu steget ut i yrkeslivet via min LIA och ser fram emot att forts√§tta utveckla dessa kunskaper och skapa fler anv√§ndbara produkter. **F√∂lj g√§rna min fortsatta resa in i detta sp√§nnande f√§lt p√• [LinkedIn](https://www.linkedin.com/in/martin-blomqvist)!**

            ---

            *Nyckelfr√•gor projektet besvarar:*
            * Kan jag **f√∂ruts√§ga ett partis tillh√∂righet** enbart genom spr√•kbruk?
            * Vilka **retoriska m√∂nster** skiljer partierna √•t i olika fr√•gor?
            * Hur f√∂r√§ndras spr√•ket √∂ver tid i **politiska debatter**?
            """
        )

        st.divider()

        st.markdown(
            """
            ### Teknisk arkitektur: en kraftfull AI-stack

            Detta projekt √§r byggt p√• en robust och modern **Python-stack**, utformad f√∂r att hantera hela AI-livscykeln ‚Äì fr√•n datainsamling till avancerad NLP och interaktiv visualisering. Jag har valt branschledande verktyg f√∂r att s√§kerst√§lla **skalbarhet, reproducerbarhet** och h√∂gsta analysprecision.
            
            ---

            ### Databehandling & modellk√§rna (the AI engine)

            | Verktyg | Funktion & analysdjup |
            | :--- | :--- |
            | **Transformers (Hugging Face)** | **K√§rnan i min NLP-l√∂sning.** Jag utnyttjar och finjusterar **state-of-the-art BERT-modellen (KB/bert-base-swedish-cased)** f√∂r banbrytande textklassificering p√• svenska. Detta m√∂jligg√∂r djup semantisk f√∂rst√•else och √∂vertr√§ffar traditionella metoder i komplexiteten hos politisk text. |
            | **Scikit-learn** | **Modellutv√§rdering & baslinjeanalys.** Anv√§nds f√∂r att etablera en p√•litlig baslinje med klassiska metoder (t.ex. TF-IDF, SVM) och rigor√∂sa evalueringar (**precision, recall, F1-score**). S√§kerst√§ller att transformer-modellerna bevisligen f√∂rb√§ttrar modellen, √§ven i sv√•ra fall s√•som vid snedvriden data. |
            | **Pandas & NumPy** | **Ryggraden i Data Science.** Dessa Python-bibliotek anv√§nds f√∂r effektiv datastrukturering, tidsserieanalys och rensning av miljontals textenheter. Hanterar komplexa ber√§kningar och transformationer n√∂dv√§ndiga f√∂r att f√∂rbereda NLP-dataset. |

            ---

            ### Webbapplikation & visualisering (the interface)

            | Verktyg | Funktion & interaktion |
            | :--- | :--- |
            | **Streamlit** | **Interaktiv webbapplikation.** Bygger den snabba och anv√§ndarv√§nliga GUI:n. G√∂r det m√∂jligt f√∂r slutanv√§ndare att **omedelbart testa AI-modeller live**, filtrera analysresultat och utforska data direkt i webbl√§saren utan n√•gon lokal installation. |
            | **Plotly, Matplotlib & Calplot** | **Dynamisk visualisering.** Ger liv √•t datan. **Plotly** skapar interaktiva grafer i applikationen, Matplotlib anv√§nds f√∂r statiska analyser, och Calplot visualiserar aktivitetsm√∂nster och trender √∂ver tid. |

            ---

            ### Datainfrastruktur & MLOps

            | Verktyg | Funktion & drifts√§kerhet |
            | :--- | :--- |
            | **PostgreSQL (via Supabase)** | **Skalbar databasl√∂sning.** Databasen hanterar effektivt √∂ver **40 000 riksdagsanf√∂randen** med komplett metadata. Den driftade PostgreSQL-instansen via Supabase s√§kerst√§ller **snabb och p√•litlig √•tkomst** till stora datavolymer. |
            | **ETL & Reproducerbarhet** | **Robust data pipeline.** ETL-pipelinen (Extract, Transform, Load) uppdaterar databasen direkt. Jag anv√§nder checkpointing, loggning och weighted sampling f√∂r att s√§kerst√§lla att modelltr√§ning √§r **reproducerbar** och att nya data automatiskt inf√∂rlivas i analysen. |
            
            ---
            
            ### Kontakt & Portfolio

            * **E-post:** [cm.blomqvist@gmail.com](mailto:cm.blomqvist@gmail.com)
            * **LinkedIn:** [Martin Blomqvist](https://www.linkedin.com/in/martin-blomqvist)
            * **GitHub:** [Martin Blomqvist](https://github.com/martinblomqvistdev)
            """
        )

    # === NYHETSRUTAN MED PARTINAMN SOM UNDERRUBRIK ===
    with news_col:
        try:
            all_articles = fetch_party_articles(articles_per_party=2)["articles"]
            if not all_articles:
                st.warning("Kunde inte h√§mta partinyheter.")
            else:
                news_html = '<div class="news-box"><h3>Senaste partinyheterna</h3>'
                
                # Gruppera artiklar per parti
                articles_by_party = {}
                for art in all_articles:
                    articles_by_party.setdefault(art["true_party"], []).append(art)
                
                for party, arts in articles_by_party.items():
                    news_html += f'<h4>{party}</h4><ul>'
                    for art in arts:
                        news_html += f'<li><a href="{art["link"]}" target="_blank">{art["title"]}</a></li>'
                    news_html += '</ul>'
                
                news_html += '</div>'
                st.markdown(news_html, unsafe_allow_html=True)
        except Exception as e:
            st.error(f"Ett fel uppstod vid h√§mtning av partinyheter: {e}")



# =====================
# Sidebar och Navigation
# =====================
with st.sidebar:
    st.title("Maktspr√•kAI")
    page = option_menu(
        menu_title=None, 
        options=PAGE_OPTIONS,
        icons=["house-fill", "search", "bar-chart-line-fill", "check2-square", "graph-up"],
        menu_icon="cast", 
        default_index=0,
        key="sidebar_main_menu"
    )
    st.divider()

    # --- CSS f√∂r nyhetsruta i sidebar utan scroll ---
    st.markdown("""
        <style>
        .news-box-sidebar {
            border: 1px solid #555;
            border-radius: 8px;
            padding: 10px;
            background-color: transparent;
            margin-bottom: 15px;
        }

        .news-box-sidebar h3 {
            margin-top: 0;
            margin-bottom: 8px;
            font-size: 0.95em;
        }

        .news-box-sidebar ul {
            list-style-type: none;
            padding-left: 0;
            margin-bottom: 0;
        }

        .news-box-sidebar li {
            margin-bottom: 6px;
            font-size: 0.85em;
        }

        .news-box-sidebar a {
            text-decoration: none;
            color: #1E90FF;
        }

        .news-box-sidebar a:hover {
            text-decoration: underline;
        }
        </style>
    """, unsafe_allow_html=True)

    # --- Inrikesnyheter i sidebar ---
    try:
        news_items = fetch_news()
        if news_items:
            news_html = '<div class="news-box-sidebar">'
            news_html += '<h3>Senaste inrikesnyheterna</h3><ul>'
            for item in news_items:
                news_html += f'<li><a href="{item["link"]}" target="_blank">{item["title"]}</a></li>'
            news_html += '</ul>'
            news_html += '<div style="text-align: right; font-size: 0.75em; margin-top: 5px;">Fr√•n <a href="https://www.svt.se/nyheter/inrikes" target="_blank">SVT Nyheter</a></div>'
            news_html += '</div>'
            st.markdown(news_html, unsafe_allow_html=True)
        else:
            st.info("Inga nyheter kunde h√§mtas just nu.")
    except Exception:
        st.error("Fel vid h√§mtning av nyheter.")


# =====================
# Huvudlogik f√∂r sidvisning
# =====================
if page == "Om projektet":
    welcome_page()

elif page == "Partiprediktion":
    st.header("Partiprediktion")
    
    # --- Introduktion ---
    st.info("""
    **Testa AI-modellen live!** Klistra in valfri text (t.ex. ett citat, pressmeddelande eller uttalande)
    fr√•n ett riksdagsparti. Eller experimentera sj√§lv med p√•hittade citat. Modellen analyserar spr√•k, ton och retorik f√∂r att f√∂ruts√§ga vilket parti
    som har skrivit texten.
    """)

    # --- Textarea och knapp ---
    user_text = st.text_area("Skriv eller klistra in ett citat h√§r:", height=150, label_visibility="collapsed")
    prediktion_placeholder = st.empty()  # Vi fyller den med resultatet senare

    if st.button("Prediktera parti"):
        if user_text.strip():
            with st.spinner("Ber√§knar‚Ä¶"):
                cleaned_text = clean_text(user_text)
                party_probs = predict_party(model, tokenizer, [cleaned_text])

                if not party_probs or not isinstance(party_probs[0], dict):
                    st.warning("Modellen returnerade inget resultat. Kontrollera input eller f√∂rs√∂k igen.")
                    party_probs = [{p: 0 for p in PARTY_ORDER}]

                party_prob_dict = party_probs[0]
                party, prob = max(party_prob_dict.items(), key=lambda x: x[1])

            # --- Visa resultatet
            with prediktion_placeholder.container():
                st.success(f"**Predikterat parti:** {party} ({prob*100:.1f}% s√§kerhet)")
                fig = px.bar(
                    x=PARTY_ORDER,
                    y=[party_prob_dict.get(p, 0) for p in PARTY_ORDER],
                    labels={"x": "Parti", "y": "Sannolikhet"},
                    text=[f"{party_prob_dict.get(p, 0)*100:.1f}%" for p in PARTY_ORDER]
                )
                st.plotly_chart(fig, config={"responsive": True})

    # --- Diskret tips/guide, centrerad och smal ---
    cols = st.columns([1, 3, 1])  # 1:3:1 ‚Üí mittkolumnen blir smalare
    with cols[1]:
        st.info("""
        üß† **Tips & exempel f√∂r att testa modellen**
        
        H√§r √§r n√•gra autentiska debattcitat du kan prova modellen p√•:
        
        - "Vi beh√∂ver st√§rka skolan och s√§kerst√§lla att alla barn f√•r samma m√∂jligheter."  
        K√§lla: [Aftonbladet Debatt](https://www.aftonbladet.se/debatt)
        - "Milj√∂n √§r v√•r tids st√∂rsta utmaning ‚Äì vi m√•ste agera nu!"  
        K√§lla: [DN Debatt](https://www.dn.se/debatt/)
        - "S√§nk skatterna f√∂r att fr√§mja f√∂retagande och innovation."  
        K√§lla: [Regeringen Debattartiklar](https://www.regeringen.se/debattartiklar/)
        
        üí° Tips:  
        - Testa p√•hittade citat eller uttalanden fr√•n offentliga personer.  
        - Anv√§nd citat fr√•n nyhetsartiklar eller offentliga dokument.  
        - Utforska hur modellen tolkar olika retoriska stilar och √§mnen.
        """)




elif page == "Spr√•kbruk & Retorik":
    st.header("J√§mf√∂r partiernas retorik")
    import datetime

    today = datetime.date.today()

    # --- Snabbval f√∂r perioder ---
    period_options = {
        "Senaste 1 m√•nad": 30,
        "Senaste 3 m√•nader": 90,
        "Senaste 6 m√•nader": 180,
        "Senaste 12 m√•nader": 365
    }
    selected_period_label = st.selectbox("V√§lj tidsperiod:", list(period_options.keys()), index=1)  # default 3 m√•nader
    days_delta = period_options[selected_period_label]
    start_date = today - datetime.timedelta(days=days_delta)
    end_date = today
    st.info(f"Visar tal fr√•n {start_date} till {end_date}")

    # --- H√§mta data ---
    with st.spinner("H√§mtar och analyserar data‚Ä¶"):
        df = fetch_speeches_historical(start_date, end_date)

    if df.empty:
        st.warning("Kunde inte hitta n√•gon data alls i databasen.")
    else:
        # --- Ordna partier ---
        df['parti'] = pd.Categorical(df['parti'], categories=PARTY_ORDER, ordered=True)

        # --- Lexikonbaserad tonanalys ---
        df_ton = apply_ton_lexicon(df, text_col="text", lexicon_path=LEXICON_PATH)
        ton_columns = [col for col in df_ton.columns if col not in ['text','parti','protokoll_datum']]

        retorik_profil = df_ton.groupby('parti', observed=False)[ton_columns].mean().reindex(PARTY_ORDER).fillna(0)
        retorik_sammansattning = retorik_profil.div(retorik_profil.sum(axis=1).replace(0,1), axis=0) * 100

        # --- Tabs ---
        tab1, tab2 = st.tabs(["Retoriskt fingeravtryck", "Rankning per kategori"])

        # --- Fingeravtryck ---
        with tab1:
            st.subheader("Partiernas retoriska fingeravtryck")
            df_plot = retorik_sammansattning.reset_index().melt(
                id_vars='parti', var_name='Kategori', value_name='Andel (%)'
            )
            df_plot['Har_data'] = df_plot['Andel (%)'] > 0

            fig_stacked_bar = px.bar(
                df_plot,
                x='parti',
                y='Andel (%)',
                color='Kategori',
                title='Sammans√§ttning av retorik per parti',
                text_auto='.1f',
                labels={'parti': 'Parti'},
                color_discrete_sequence=px.colors.qualitative.Set3
            )

            # Markera partier utan data
            for parti in PARTY_ORDER:
                if not df_plot[df_plot['parti']==parti]['Har_data'].any():
                    fig_stacked_bar.add_scatter(
                        x=[parti],
                        y=[0],
                        mode='markers',
                        marker=dict(color='lightgrey', size=20),
                        showlegend=False,
                        name=f"{parti} (ingen data)"
                    )

            fig_stacked_bar.update_layout(xaxis={'categoryorder':'array', 'categoryarray': PARTY_ORDER})
            st.plotly_chart(fig_stacked_bar, config={"responsive": True})

        # --- Rankning per kategori ---
        with tab2:
            st.subheader("Rankning per retorisk kategori")
            category_to_rank = st.selectbox("V√§lj retorisk kategori:", sorted(retorik_profil.columns))
            if category_to_rank:
                source_df = retorik_profil[[category_to_rank]].copy()
                max_value = source_df[category_to_rank].max()
                source_df['Rankning'] = (source_df[category_to_rank] / max_value * 100) if max_value > 0 else 0
                ranked_df = source_df.sort_values(by='Rankning', ascending=True)
                fig_bar = px.bar(
                    ranked_df,
                    x='Rankning',
                    y=ranked_df.index,
                    orientation='h',
                    labels={"y": "Parti", "x": "Relativ po√§ng (Ledaren = 100)"},
                    text_auto='.1f',
                    title=f"Relativ rankning - {category_to_rank}"
                )
                fig_bar.update_layout(yaxis={'categoryorder':'array', 'categoryarray': ranked_df.index.tolist()})
                st.plotly_chart(fig_bar, config={"responsive": True})

        # --- WordClouds ---
        st.divider()
        st.subheader("Vanligaste orden per parti")
        cols = st.columns(4)
        for i, party in enumerate(PARTY_ORDER):
            with cols[i % 4]:
                raw_text_blob = " ".join(df[df["parti"]==party]["text"].dropna().tolist())
                cleaned_text_for_cloud = preprocess_for_wordcloud(raw_text_blob)
                if cleaned_text_for_cloud:
                    try:
                        wc = WordCloud(width=400, height=300, background_color="white", collocations=False).generate(cleaned_text_for_cloud)
                        st.write(f"**{party}**")
                        fig_wc, ax = plt.subplots(figsize=(4,3))
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off")
                        st.pyplot(fig_wc)
                        plt.close(fig_wc)
                    except Exception:
                        st.error(f"Kunde inte generera moln f√∂r {party}.")
                else:
                    st.write(f"**{party}** (F√∂r lite text)")


elif page == "Evaluering":
    st.header("Automatisk Testb√§nk: Prediktion p√• partiernas egna texter")
    
    # === NY, MER F√ñRKLARANDE INFO-BOX ===
    st.info("""
    H√§mta och evaluera de senaste texterna direkt fr√•n riksdagspartiernas hemsidor. 
    **Notera:** Antalet funna artiklar kan vara l√§gre √§n det beg√§rda d√• vissa partier har inaktiva RSS-fl√∂den 
    eller att deras senaste inl√§gg √§r videoklipp som s√•llats bort av kvalitetsfiltret.
    """)

    num_per_party = st.slider(
        "Antal senaste artiklar att h√§mta per parti", 1, 5, 2
    )
    
    show_debug = st.checkbox("Visa fels√∂kningslogg")

    if 'fetch_results' not in st.session_state:
        st.session_state.fetch_results = {"articles": [], "log": [], "found_parties": set()}

    if st.button(f"H√§mta & Evaluera upp till {num_per_party * 8} partitexter"):
        with st.spinner(f"H√§mtar och analyserar texter... Detta kan ta en stund."):
            st.session_state.fetch_results = fetch_party_articles(articles_per_party=num_per_party)
            # Spara vilka partier vi faktiskt hittade artiklar f√∂r
            st.session_state.fetch_results["found_parties"] = {a['true_party'] for a in st.session_state.fetch_results.get("articles", [])}
        st.rerun()

    articles_to_analyze = st.session_state.fetch_results.get("articles", [])
    debug_log = st.session_state.fetch_results.get("log", [])
    found_parties = st.session_state.fetch_results.get("found_parties", set())

    # Om vi har klickat p√• knappen, visa en sammanfattning
    if debug_log: 
        # === NY SAMMANFATTNING √ñVER RESULTATET ===
        st.subheader("Resultat")
        
        missing_parties = set(PARTY_ORDER) - found_parties
        if missing_parties:
            # G√∂r om set till en snygg str√§ng, t.ex. "S, C, KD, MP"
            missing_parties_str = ", ".join(sorted(list(missing_parties)))
            st.warning(f"**Kunde inte hitta giltiga artiklar f√∂r:** {missing_parties_str}")

        if articles_to_analyze:
            # (Hela din existerande kod f√∂r att visa metrics och tabell)
            results = []
            for article in articles_to_analyze:
                cleaned_for_model = clean_text(article['content'])
                party_probs = predict_party(model, tokenizer, [cleaned_for_model])
                predicted_party = max(party_probs[0].items(), key=lambda x: x[1])[0]
                results.append({
                    "Titel": article['title'], "Sant parti": article['true_party'], "Modellens gissning": predicted_party,
                    "Korrekt?": "‚úÖ" if article['true_party'] == predicted_party else "‚ùå", "L√§nk": article['link']
                })
            results_df = pd.DataFrame(results)
            correct_count = (results_df["Korrekt?"] == "‚úÖ").sum()
            total_count = len(results_df)
            accuracy = (correct_count / total_count) * 100 if total_count > 0 else 0

            col1, col2, col3 = st.columns(3)
            col1.metric("Antal texter analyserade", f"{total_count}")
            col2.metric("Antal korrekta gissningar", f"{correct_count}")
            col3.metric("Tr√§ffs√§kerhet", f"{accuracy:.1f}%")
            st.divider()
            st.dataframe(results_df, column_config={"L√§nk": st.column_config.LinkColumn("L√§nk", display_text="√ñppna artikel")})
        else:
            st.error("Inga giltiga artiklar alls kunde hittas fr√•n n√•gon av partiernas fl√∂den.")

    if show_debug:
        st.divider()
        st.subheader("Fels√∂kningslogg")
        st.code("\n".join(debug_log), language="text")

elif page == "Historik":
    st.header("Analysera retorikens utveckling √∂ver tid")

    # --- 1. Definiera tidsgr√§ns och ladda data ---
    MAX_YEARS = 10 
    today = date.today()
    START_DATE_LIMIT = today - timedelta(days=365 * MAX_YEARS) 

    # L√§s lexikon och h√§mta kategorier
    lex_df_temp = pd.read_csv(LEXICON_PATH)
    ton_columns = lex_df_temp['kategori'].unique().tolist()

    # Anv√§ndarval av kategori
    category_to_track = st.selectbox(
        "V√§lj retorisk kategori att f√∂lja √∂ver tid:", 
        sorted(ton_columns),
        key="historic_category_select"
    )

    with st.spinner(f"Analyserar historisk data f√∂r alla partier i kategorin '{category_to_track}' tio √•r tillbaka..."):
        
        # H√§mta ALL data inom den maximala tidsperioden
        df_all_data = fetch_speeches_historical("2015-01-01", today)
        
        if df_all_data.empty:
            st.warning(f"Hittade ingen data alls inom den valda tidsgr√§nsen ({START_DATE_LIMIT.year} till {today.year}).")
            st.stop()

        # 2. Robust hantering av DATUM
        df_all_data['protokoll_datum'] = pd.to_datetime(df_all_data['protokoll_datum'], errors='coerce') 
        valid_dates_df = df_all_data.dropna(subset=['protokoll_datum'])

        # Debug-info: visa b√•de efterfr√•gat och faktiskt intervall
        requested_range = f"{START_DATE_LIMIT} ‚Üí {today}"
        if not valid_dates_df.empty:
            min_date = valid_dates_df['protokoll_datum'].min().strftime('%Y-%m-%d')
            max_date = valid_dates_df['protokoll_datum'].max().strftime('%Y-%m-%d')
            st.info(f"Efterfr√•gad period: **{requested_range}**\n\nData i databasen t√§cker: **{min_date} ‚Üí {max_date}**")
        else:
            st.warning("Hittade inga giltiga datum i den h√§mtade datan efter rensning.")
            st.stop()

        # 3. Aggregera till KURVOR (per √ÖR)
        df_ton = apply_ton_lexicon(valid_dates_df, text_col="text", lexicon_path=LEXICON_PATH)
        df_ton['√Ör'] = df_ton['protokoll_datum'].dt.to_period('Y') 
        df_plot_yearly = df_ton.groupby(['parti', '√Ör'], observed=False)[category_to_track].mean().reset_index()
        df_plot_yearly['√Ör'] = df_plot_yearly['√Ör'].astype(str).str.split('-').str[0].astype(int) 
        unique_years = sorted(df_plot_yearly['√Ör'].unique())

        # Konvertera till procent av partiets tal
        df_plot_yearly[category_to_track] = df_plot_yearly[category_to_track] * 100

        # 4. Visualisering
        st.subheader(f"Utveckling av retoriken: '{category_to_track}'")
        st.markdown(f"Visar trenden f√∂r de senaste {MAX_YEARS} √•ren med √•rlig uppl√∂sning.")

        fig = px.line(
            df_plot_yearly,
            x="√Ör",
            y=category_to_track,
            color="parti",
            markers=True,
            title=f"Trend: '{category_to_track}' per parti √∂ver tid (√Örlig uppl√∂sning)"
        )

        fig.update_xaxes(
            title_text="√Ör", 
            tickvals=unique_years, 
            ticktext=[str(year) for year in unique_years], 
            showgrid=True
        )
        fig.update_yaxes(
            title_text=f"% av partiets tal med kategori '{category_to_track}'", 
            range=[df_plot_yearly[category_to_track].min() * 0.9,
                   df_plot_yearly[category_to_track].max() * 1.1]
        )
        fig.update_traces(hovertemplate='%{y:.1f}% av partiets tal')

        st.plotly_chart(fig, config={"responsive": True})

    st.divider()

    # --- WordClouds per parti ---
    st.subheader("J√§mf√∂r partiernas vanligaste ord")
    st.markdown("V√§lj en tidsperiod nedan f√∂r att se ordmoln sida vid sida.")

    time_periods_for_cloud = {
        "Senaste 10 √•ren": (today - timedelta(days=365*10), today),
        "Senaste 5 √•ren": (today - timedelta(days=365*5), today),
        "Senaste 2 √•ren": (today - timedelta(days=365*2), today),
        "Senaste √•ret": (today - timedelta(days=365), today),
        "Senaste 90 dagarna": (today - timedelta(days=90), today),
        "Senaste 30 dagarna": (today - timedelta(days=30), today)
    }
    
    period_options_reversed = list(time_periods_for_cloud.keys())[::-1]
    period_for_cloud = st.selectbox(
        "V√§lj period f√∂r ordmolnen:",
        period_options_reversed,
        index=0,
        key="all_party_period_select"
    )

    start, end = time_periods_for_cloud[period_for_cloud]
    df_all_data_cloud = fetch_speeches_historical(start, end)[['text', 'parti']]

    if df_all_data_cloud.empty:
        st.warning(f"Ingen data hittades f√∂r ordmoln under '{period_for_cloud}'.")
    else:
        st.markdown(f"**Visar ordmoln baserat p√• tal under perioden: {period_for_cloud}**")
        cols = st.columns(4)

        for i, party in enumerate(PARTY_ORDER):
            with cols[i % 4]:
                df_party = df_all_data_cloud[df_all_data_cloud['parti'] == party]
                
                if df_party.empty:
                    st.write(f"**{party}** (Ingen data)")
                    continue

                raw_text_blob = " ".join(df_party["text"].dropna().tolist())
                cleaned_text_for_cloud = preprocess_for_wordcloud(raw_text_blob)

                if cleaned_text_for_cloud:
                    try:
                        wc = WordCloud(
                            width=400,
                            height=300,
                            background_color="white",
                            collocations=False
                        ).generate(cleaned_text_for_cloud)
                        st.write(f"**{party}**")
                        fig_wc, ax = plt.subplots(figsize=(4, 3))
                        ax.imshow(wc, interpolation='bilinear')
                        ax.axis("off")
                        st.pyplot(fig_wc, bbox_inches='tight', dpi=fig_wc.dpi)
                        plt.close(fig_wc)
                    except Exception:
                        st.error(f"Kunde inte generera moln f√∂r {party}.")
                else:
                    st.write(f"**{party}** (F√∂r lite text)")
